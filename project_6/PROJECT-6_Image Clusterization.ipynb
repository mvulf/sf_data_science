{"cells":[{"cell_type":"markdown","metadata":{"id":"p4Ujr-rfVLuX"},"source":["# <center> Кластеризация изображений транспортных средств"]},{"cell_type":"markdown","metadata":{"id":"Ur9NEBX0VLua"},"source":["## Постановка задачи"]},{"cell_type":"markdown","metadata":{"id":"nxMMi_TwVLub"},"source":["<center> <img src=https://i.ibb.co/t8DvkyB/smart-city-image-1.jpg align=\"right\" width=\"300\"/> </center>\n","<center> <img src=https://i.ibb.co/qYkWNVh/smart-city-image-3.jpg align=\"right\" width=\"300\"/> </center>\n","\n","\n","Один из ключевых проектов IntelliVision — Smart City/Transportation, система, обеспечивающая безопасность дорожного движения и более эффективную работу парковок. С помощью Smart City/Transportation можно контролировать сигналы светофоров и соблюдение ограничений скорости, определять виды транспортных средств, распознавать номерные знаки, считать автомобили и людей.\n","\n","В основе всех перечисленных возможностей проекта лежит CV (Computer Vision, компьютерное зрение). Чтобы их реализовать, компания использует модели, для обучения которых применяются огромные размеченные датасеты с изображениями транспортных средств. Однако система работает в режиме реального времени и с каждым днём данных становится всё больше. Алгоритм нуждается в постоянной модернизации и должен учитывать множество факторов.\n","\n","Для модификации и повышения эффективности системы Smart City/Transportation команде необходимо автоматизировать определение дополнительных параметров авто на изображении:\n","\n","* тип автомобиля (кузова),\n","* ракурс снимка (вид сзади/спереди),\n","* цвет автомобиля,\n","* другие характеристики.\n","\n","Также необходимо автоматизировать поиск выбросов в данных (засветы и блики на изображениях, изображения, на которых отсутствуют автомобили и т. д.).\n","\n","К сожалению, у компании нет комплексной модели, которая могла бы одновременно находить на изображении автомобиль и определять все нужные параметры. Её нужно построить, однако многокомпонентная разметка новых данных по всем этим параметрам — очень трудозатратное занятие, которое стоит больших денег.\n","\n","При решении задачи разметки данных у команды возникла гипотеза, которая нуждается в исследовании.\n","\n","\n","**Гипотеза:** разметку исходных данных можно эффективно провести с помощью методов кластеризации. \n","\n","\n","**В чём идея?**\n","\n","*Давайте будем использовать небольшой набор моделей свёрточных нейронных сетей, обученных на различных датасетах и решающих различные задачи от классификации изображений по цвету до классификации типов транспортных средств, пропустим нашу базу изображений через каждую модель, но возьмём не выходной результат модели, а только промежуточное представление признаков (дескриптор), полученное на свёрточных слоях сети.*\n","\n","*Выполним такую операцию для всех изображений из набора данных, на основе полученных дескрипторов кластеризуем изображения, проинтерпретируем полученные кластеры и попробуем найти в них необходимую информацию.*\n","\n","Теперь, когда мы обсудили гипотезу, перейдём к постановке задачи.\n","\n","<center> <img src=https://i.ibb.co/hLcBpZF/2023-03-27-12-11-17.png align=\"right\" width=\"500\"/> </center>\n","\n","У вас будет набор из 416 314 изображений транспортных средств различных типов, цветов и снятых с разных ракурсов.\n","\n","Команда IntelliVision уже обработала свой набор данных с помощью нескольких моделей глубокого обучения (свёрточных нейронных сетей) и получила четыре варианта вектора признаков (дескрипторов) для каждого изображения.\n","\n","**Ваша задача** — используя готовые дескрипторы, разбить изображения на кластеры и проинтерпретировать каждый из них. Для всех вариантов дескрипторов нужно применить несколько алгоритмов кластеризации и сравнить полученные результаты. Сравнивать можно на основе метрик, визуализаций плотностей кластеров и по тому, насколько хорошо интерпретируются кластеры.\n","\n","Дополнительная подзадача — найти выбросы среди изображений. Это могут быть изображения плохого качества, изображения с бликами или изображения, на которых нет транспортных средств и т. д.\n","\n","Бизнес-задача: исследовать возможность применения алгоритмов кластеризации для разметки новых данных и поиска выбросов.\n","\n","Техническая задача для вас как для специалиста в Data Science: построить модель кластеризации изображений на основе дескрипторов, выделяемых с помощью различных архитектур нейронных сетей, проинтерпретировать полученные результаты и выбрать модель или комбинацию моделей, которая выделяет наиболее пригодные для интерпретации признаки.\n","\n","**Ваши основные цели:**\n","1. Для каждого типа дескрипторов необходимо:\n","    * выполнить предобработку дескрипторов;\n","    * произвести кластеризацию изображений на основе их дескрипторов, подобрав алгоритм и параметры кластеризации;\n","    * сделать визуализацию полученных кластеров в 2D- или 3D-пространстве;\n","    * проинтерпретировать полученные кластеры — в паре предложений сформулировать, какие изображения попали в каждый из кластеров.\n","2. Сравнить между собой полученные кластеризации для каждого типа дескрипторов (по метрикам, визуализации и результатам интерпретации).\n","3. Выполнить автоматизированный поиск выбросов среди изображений на основе дескрипторов.\n","4. Дополнительная задача (не оценивается): попробовать воспользоваться смесью дескрипторов, полученных различными моделями, и проинтерпретировать полученные результаты.\n","\n","**Примечание.** При выборе алгоритма кластеризации следует ориентироваться на внутренние метрики, а именно на индекс Калински — Харабаса (`calinski_harabasz_score`) и индекс Дэвиса — Болдина (`davies_bouldin_score`), а также на интерпретируемость кластеров и визуализацию."]},{"cell_type":"markdown","metadata":{"id":"DoE6yUI5VLud"},"source":["## Данные и их описание"]},{"cell_type":"markdown","metadata":{"id":"u3kWYQ_UVLud"},"source":["Исходная папка с данными имеет следующую структуру:\n","\n","```\n","IntelliVision_case\n","├─descriptors\n","    └─efficientnet-b7.pickle\n","    └─osnet.pickle\n","    └─vdc_color.pickle\n","    └─vdc_type.pickle\n","├─row_data\n","    └─veriwild.zip\n","├─images_paths.csv \n","```\n","\n","Давайте разберёмся в ней:\n","\n","* В папке `descriptors` содержатся дескрипторы, полученные для каждого из изображений с помощью соответствующих нейронных сетей, в формате numpy-массивов, сохранённых в файлах pickle:\n","    * `efficientnet-b7.pickle` — дескрипторы, выделенные моделью классификации с архитектурой EfficientNet версии 7. Эта модель является свёрточной нейронной сетью, предобученной на на датасете ImageNet, в котором содержатся изображения более 1000 различных классов. Эта модель при обучении не видела датасета veriwiId. \n","\n","    * `osnet.pickle` — дескрипторы, выделенные моделью OSNet, обученной для детектирования людей, животных и машин. Модель не обучалась на исходном датасете veriwiId.\n","\n","    * `vdc_color.pickle` — дескрипторы, выделенные моделью регрессии для определения цвета транспортных средств в формате RGB. Частично обучена на исходном датасете veriwild.\n","    \n","    * `vdc_type.pickle` — дескрипторы, выделенные моделью классификации транспортных средств по типу на десяти классах. Частично обучена на исходном датасете veriwild.\n","\n","* В папке `row_data` содержится zip-архив с исходными изображениями автомобилей. Распакуйте его содержимое в папку row_data. Архив содержит десять папок с изображениями, пронумерованных от 1 до 10. Каждая папка содержит подпапки, обозначенные пятизначными цифрами, например 36191. \n","\n","В каждой из таких подпапок содержатся фотографии одного конкретного автомобиля с разных ракурсов, снятые с помощью дорожных видеокамер.\n","\n","* В файле `images_paths.csv` представлен список из полных путей до изображений. Он пригодится вам при анализе изображений, попавших в определённый кластер.\n"]},{"cell_type":"markdown","metadata":{"id":"BUyPkBq9VRZH"},"source":["Импорт базовых библиотек:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BXq-4lEcVVHC"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import plotly.graph_objs as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","\n","import warnings \n","\n","from IPython.display import display, HTML\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.rcParams[\"patch.force_edgecolor\"] = True\n","\n","import pickle\n","\n","import cupy as cp\n","\n","from cuml import UMAP, TSNE\n","from cuml.decomposition import PCA\n","from cuml.preprocessing import StandardScaler, MinMaxScaler\n","from cuml.cluster import KMeans, AgglomerativeClustering, DBSCAN\n","\n","from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score"]},{"cell_type":"markdown","metadata":{"id":"-sMDS8iFVLue"},"source":["## 1. Знакомство со структурой данных"]},{"cell_type":"markdown","metadata":{"id":"dZxYrnAZVLuf"},"source":["Прочитайте numpy-массивы из предоставленных pickle-файлов.\n","\n","**Примечание** Для удобства дальнейшей работы вы можете составить четыре DataFrame с путями до изображений и соответствующими им дескрипторами.\n","\n","Посмотрите на размерности каждой из четырёх заданных матриц и сравните использованные модели глубокого обучения по размерностям выходных дескрипторов изображений. \n"]},{"cell_type":"markdown","metadata":{},"source":["### Решение:"]},{"cell_type":"markdown","metadata":{},"source":["Организуем класс для загрузки требуемого датасета (чтобы не перегружать оперативную память или память видеокарты). См. класс **\"DataLoader\"**\n","\n","Затем поочерёдно откроем датасеты для получения информации о них."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of \"image paths\": (416314, 1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paths</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>veriwild\\1\\00001\\000001.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>veriwild\\1\\00001\\000002.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>veriwild\\1\\00001\\000003.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>veriwild\\1\\00001\\000004.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>veriwild\\1\\00001\\000005.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         paths\n","0  veriwild\\1\\00001\\000001.jpg\n","1  veriwild\\1\\00001\\000002.jpg\n","2  veriwild\\1\\00001\\000003.jpg\n","3  veriwild\\1\\00001\\000004.jpg\n","4  veriwild\\1\\00001\\000005.jpg"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Descriptor \"efficientnet-b7\":\n","\tShape: (416314, 2560)\n","Descriptor \"osnet\":\n","\tShape: (416314, 512)\n","Descriptor \"vdc_color\":\n","\tShape: (416314, 128)\n","Descriptor \"vdc_type\":\n","\tShape: (416314, 512)\n"]}],"source":["class DataLoader():\n","    def __init__(\n","        self,\n","        descriptor_names = [\n","            'efficientnet-b7', 'osnet', 'vdc_color', 'vdc_type'\n","        ],\n","        descriptor_folder = 'data/descriptors/',\n","        image_paths = 'data/images_paths.csv',\n","        verbose:bool = True,\n","    ):\n","        \"\"\"Class for data loading (descriprots and image_paths)\n","\n","        Args:\n","            descriptor_names (list): List of descriptor names. Defaults to ['efficientnet-b7', 'osnet', 'vdc_color', 'vdc_type'].\n","            descriptor_folder (str, optional): Folder where descriptors kept. Defaults to 'data/descriptors/'.\n","            image_paths (str, optional): Path to \"image_paths\" dataset. Defaults to 'data/images_paths.csv'.\n","            verbose (bool): Print additional messages. Defaults to True\n","            \n","        \"\"\"\n","        self._descriptor_folder = descriptor_folder\n","        self.descriptor_names = descriptor_names\n","        self.image_paths = pd.read_csv(image_paths)\n","        self._active_descriptor = None\n","        self._active_descriptor_name = None\n","        self._active_image_paths = None\n","        self.verbose = verbose\n","        if self.verbose:\n","            # Show image paths details\n","            print('Shape of \"image paths\":', self.image_paths.shape)\n","            display(self.image_paths.head())\n","    \n","    def __del__(self):\n","        del self._active_descriptor\n","    \n","    def load_descriptor(self, name:str, dtype:str='cupy'):\n","        \"\"\"Load descriptor with stated \"name\" in RAM or GPU (see \"dtype\")\n","\n","        Args:\n","            name (str): Name of the descriptor to load\n","            dtype (str): Select type of data:\n","            - 'numpy' - numpy.ndarray format (keep data on RAM);\n","            - 'cupy' - cupy.ndarray format (keep data on GPU).\n","            Defaults to 'cupy'.\n","\n","        Raises:\n","            TypeError: If data type is not in [\"numpy\", \"cupy\"]\n","        \"\"\"\n","        with open(\n","            self._descriptor_folder + name + '.pickle',\n","            'rb' # read binary\n","        ) as pkl_file:\n","            self._active_descriptor = pickle.load(pkl_file)\n","            self._active_descriptor_name = name\n","            if dtype.strip().lower() == 'numpy':\n","                pass\n","            elif dtype.strip().lower() == 'cupy':\n","                self._active_descriptor = cp.asarray(self._active_descriptor)\n","            else:\n","                raise TypeError('Wrong data_type. Only \"numpy\" and \"cupy\" allowed')\n","        self._active_image_paths = self.image_paths\n","    \n","    \n","    def load_descriptor_random(\n","        self, \n","        name:str,\n","        dtype:str='cupy',\n","        data_fraction:float=0.5,\n","        random_state:int=None,\n","    ):\n","        \"\"\"Load random part of the descriptor with stated \"name\" in RAM or GPU (see \"dtype\")\n","        and with setted fraction\n","\n","        Args:\n","            name (str): Name of the descriptor to load\n","            dtype (str): Select type of data:\n","            - 'numpy' - numpy.ndarray format (keep data on RAM);\n","            - 'cupy' - cupy.ndarray format (keep data on GPU).\n","            Defaults to 'cupy'.\n","            data_fraction (float): Fraction of the descriptor to load. Defaults to 0.5.\n","            random_state (int, optional): Set random state for the random generator. Defaults to None.\n","\n","        Raises:\n","            ValueError: Data fraction must be in the range [0, 1]\n","        \"\"\"\n","        if data_fraction < 0.0 or data_fraction > 1.0:\n","            raise ValueError('data_fraction must be in the range [0, 1]')\n","        \n","        self.load_descriptor(name, dtype)\n","        \n","        # Get required row count\n","        row_cnt = np.ceil(\n","            data_fraction * self._active_descriptor.shape[0]\n","        ).astype(int)\n","        \n","        rng = np.random.default_rng(random_state)\n","        indexes = rng.choice(\n","            self._active_descriptor.shape[0], \n","            size=row_cnt,\n","            replace=False\n","        )\n","        indexes.sort()\n","        \n","        # Save active descriptor and related image paths\n","        self._active_descriptor = self._active_descriptor[indexes,:]\n","        self._active_image_paths = self.image_paths.iloc[indexes,:]\n","        \n","    \n","    @property\n","    def active_descriptor(self):\n","        \"\"\"Return active descriptor and its name\n","\n","        Returns:\n","            typle: (descriptor name, descriptor)\n","        \"\"\"\n","        return (self._active_descriptor_name, self._active_descriptor)\n","    \n","    \n","    @property\n","    def active_image_paths(self):\n","        return self._active_image_paths\n","    \n","    \n","    def print_descriptor_info(self):\n","        \"\"\"Print active descriptor name and shape\n","        \"\"\"\n","        descriptor_name, descriptor = self.active_descriptor\n","        print(f'Descriptor \"{descriptor_name}\":')\n","        print(f'\\tShape: {descriptor.shape}')\n","\n","\n","\n","loader = DataLoader()\n","# Load datasets to show info\n","for descriptor in loader.descriptor_names:\n","    loader.load_descriptor(descriptor, dtype='numpy')\n","    loader.print_descriptor_info()"]},{"cell_type":"markdown","metadata":{},"source":["**ВЫВОД:**\n","\n","Датасет с путями, а также все дескрипторы, содержат одинаковое количество строк: 416314.\n","Датасет с путями содержит одну колонку 'paths', содержащую относительный путь до картинок.\n","\n","Для этих картинок имеются четыре дескриптора (дескриптор - векторизованный выход после сверточных слоёв).\n","Самый большой дескриптор имеет модель классификации с архитектурой **EfficientNet** версии 7.\n","Следом за ней (по размеру дескриптора) идут модель **OSNet** и модель классификации транспортных средств по типу **'vdc_type'**.\n","Меньше всего размер дескриптора, отвечающего за цвет транспортного средства **'vdc_color'**.\n"]},{"cell_type":"markdown","metadata":{"id":"Eg2mLQRJVLuh"},"source":["## 2. Преобразование, очистка и анализ данных"]},{"cell_type":"markdown","metadata":{"id":"xHVqC_fIVLuh"},"source":["Признаки, найденные с помощью некоторых моделей, исчисляются тысячами, что довольно много, учитывая общее количество наблюдений.\n","\n","Как вы понимаете, производить кластеризацию на таком большом количестве признаков, которые были сформированы исходными моделями глубокого обучения, довольно сложно и затратно по времени. К тому же, многие признаки, найденные моделями на изображениях, могут быть сильно скоррелированы между собой.\n","\n","Понизьте размерность исходных дескрипторов с помощью соответствующих методов. Можно уменьшить размерность входных данных до 100 или 200 признаков — этого будет достаточно, чтобы произвести кластеризацию, однако рекомендуем вам самостоятельно подобрать необходимое количество компонент в новом пространстве признаков.\n","\n","Также позаботьтесь о масштабе признаков, воспользовавшись стандартизацией и нормализацией. После кластеризации определите, какой вариант масштабирования более успешен для каждого варианта дескрипторов.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Решение"]},{"cell_type":"markdown","metadata":{},"source":["Подберём необходимое количество компонент для каждого дескриптора по объясняемому разбросу (explained_variance_ratio_) в предположении, что требуется минимум от 50 до 75 % объяснения данных.\n","\n","Брать максимум мы не будем ввиду ограничений памяти в GPU."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ДЕСКРИПТОР \"efficientnet-b7\"\n","Количество признаков: 50 \t\"Объясняемый\" разброс: 0.343\n","Количество признаков: 100 \t\"Объясняемый\" разброс: 0.466\n","Количество признаков: 150 \t\"Объясняемый\" разброс: 0.557\n","Количество признаков: 200 \t\"Объясняемый\" разброс: 0.628\n","Количество признаков: 250 \t\"Объясняемый\" разброс: 0.685\n","Количество признаков: 300 \t\"Объясняемый\" разброс: 0.730\n","Количество признаков: 350 \t\"Объясняемый\" разброс: 0.762\n","\n","ДЕСКРИПТОР \"osnet\"\n","Количество признаков: 50 \t\"Объясняемый\" разброс: 0.846\n","Количество признаков: 100 \t\"Объясняемый\" разброс: 0.925\n","Количество признаков: 150 \t\"Объясняемый\" разброс: 0.955\n","Количество признаков: 200 \t\"Объясняемый\" разброс: 0.970\n","Количество признаков: 250 \t\"Объясняемый\" разброс: 0.980\n","Количество признаков: 300 \t\"Объясняемый\" разброс: 0.987\n","Количество признаков: 350 \t\"Объясняемый\" разброс: 0.992\n","\n","ДЕСКРИПТОР \"vdc_color\"\n","Количество признаков: 50 \t\"Объясняемый\" разброс: 0.870\n","Количество признаков: 100 \t\"Объясняемый\" разброс: 0.963\n","Достигнут предел по количеству признаков\n","\n","ДЕСКРИПТОР \"vdc_type\"\n","Количество признаков: 50 \t\"Объясняемый\" разброс: 0.966\n","Количество признаков: 100 \t\"Объясняемый\" разброс: 0.983\n","Количество признаков: 150 \t\"Объясняемый\" разброс: 0.990\n","Количество признаков: 200 \t\"Объясняемый\" разброс: 0.993\n","Количество признаков: 250 \t\"Объясняемый\" разброс: 0.996\n","Количество признаков: 300 \t\"Объясняемый\" разброс: 0.997\n","Количество признаков: 350 \t\"Объясняемый\" разброс: 0.998\n","\n"]}],"source":["for name in loader.descriptor_names:\n","    loader.load_descriptor_random(\n","        name, \n","        random_state=42,\n","    )\n","    descriptor_name, descriptor = loader.active_descriptor\n","\n","    print(f'ДЕСКРИПТОР \"{descriptor_name}\"')\n","\n","    for n_components in np.arange(50, 400, 50):\n","        if n_components > descriptor.shape[1]:\n","            print('Достигнут предел по количеству признаков')\n","            break\n","        pca_cu = PCA(n_components=n_components, random_state=42)\n","        effnt_cu = pca_cu.fit_transform(descriptor)\n","        print('Количество признаков:', n_components,\n","            f'\\t\"Объясняемый\" разброс: {pca_cu.explained_variance_ratio_.sum():.3f}'\n","        )\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Создадим класс `DataKeeper` для хранения дескрипторов с уменьшенным количеством признаков (с помощью метода главных компонент), а также их масштабированных версий (StandardScaler и MinMacScaler)"]},{"cell_type":"markdown","metadata":{},"source":["Примем следующие значения количества признаков в PCA из условия, что \"объясняемый\" разброс не увеличивается значительно с последующим увеличением (см. атрибут `n_components_dict` в классе `DataKeeper`):"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class DataKeeper():\n","    def __init__(\n","        self,\n","        n_components_dict:dict = {\n","            \"efficientnet-b7\": 250,\n","            \"osnet\": 150,\n","            \"vdc_color\": 100,\n","            \"vdc_type\": 150,\n","        },\n","        loader_verbose = False,\n","    ):\n","        \"\"\"DataKeeper keeps active PCA-reduced descriptor and its Standard- and Norm- scaled version\n","\n","        Args:\n","            n_components_dict (dict): Number of PCA-components for each dataframe. Defaults to {\n","                    \"efficientnet-b7\": 250, \n","                    \"osnet\": 150, \n","                    \"vdc_color\": 100, \n","                    \"vdc_type\": 150, \n","                }.\n","            loader_verbose (bool): Print additional information? Defaults to False.\n","        \"\"\"\n","        self.loader = DataLoader(verbose=loader_verbose)\n","        self.n_components_dict = n_components_dict\n","        self.active_descriptor_PCA = None\n","        self.active_pca:PCA = None\n","        \n","        # StandardScaler\n","        self.active_descriptor_std = None\n","        self.active_std_scaler = None\n","        \n","        # MinMaxScaler\n","        self.active_descriptor_norm = None\n","        self.active_norm_scaler = None\n","        \n","    \n","    def get_PCA_descriptor(\n","        self,\n","        random_state:int=None,\n","    ):\n","        \"\"\"Get descriptor with reduced features (by PCA)\n","\n","        Args:\n","            random_state (int): Random state for the PCA. Defaults to None.\n","        \"\"\"\n","        self.active_pca = PCA(\n","            n_components=self.n_components_dict[\n","                self.loader._active_descriptor_name\n","            ], \n","            random_state=random_state\n","        )\n","        self.active_descriptor_PCA = self.active_pca.fit_transform(\n","            self.loader._active_descriptor\n","        )\n","        # Free space of the original desciptor\n","        self.loader._active_descriptor = None\n","    \n","    \n","    def get_std_scaled_descriptor(self):\n","        \"\"\"Get Standard scaled data\n","        \"\"\"\n","        self.active_std_scaler = StandardScaler()\n","        self.active_descriptor_std = self.active_std_scaler.fit_transform(\n","            self.active_descriptor_PCA\n","        )\n","    \n","    \n","    def get_norm_scaled_descriptor(self):\n","        \"\"\"Get MinMax scaled data\n","        \"\"\"\n","        self.active_norm_scaler = MinMaxScaler()\n","        self.active_descriptor_norm = self.active_norm_scaler.fit_transform(\n","            self.active_descriptor_PCA\n","        )\n","    \n","    \n","    def get_scaled_descriptors(\n","        self,\n","    ):\n","        \"\"\"Get Norm- and Standard- scaled data\n","        \"\"\"\n","        self.get_norm_scaled_descriptor()\n","        self.get_std_scaled_descriptor()"]},{"cell_type":"markdown","metadata":{},"source":["Рассмотрим работу PCA, MinMaxScaler и StandardScaler на примере \"первого\" в списке дескриптора:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Normalized descriptor: (208157, 250)\n","Standartized descriptor: (208157, 250)\n"]}],"source":["if 'loader' in locals():\n","    del loader # Remove data loader to free GPU storage\n","cp._default_memory_pool.free_all_blocks()\n","\n","data_keeper = DataKeeper()\n","\n","data_keeper.loader.load_descriptor_random(\n","    data_keeper.loader.descriptor_names[0],\n","    random_state=42\n",")\n","data_keeper.get_PCA_descriptor(random_state=42)\n","data_keeper.get_scaled_descriptors()\n","\n","print(f'Normalized descriptor: {data_keeper.active_descriptor_norm.shape}')\n","print(f'Standartized descriptor: {data_keeper.active_descriptor_std.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["В следующем разделе будет выбран наилучший метод масштабирования."]},{"cell_type":"markdown","metadata":{"id":"8wczjX5kVLui"},"source":["## 3. Моделирование и оценка качества модели"]},{"cell_type":"markdown","metadata":{"id":"ES-rdlkYVLui"},"source":["### 3.1. Кластеризация изображений"]},{"cell_type":"markdown","metadata":{"id":"HfdwiOyWVLui"},"source":["После предобработки исходных данных произведите кластеризацию для каждого набора дескрипторов.\n","\n","Для решения задачи используйте несколько различных методов, подобрав оптимальное количество кластеров для каждого метода и варианта дескрипторов.\n","\n","В качестве метрики для подбора оптимального количества кластеров используйте внутренние меры индекс Калински — Харабаса (`calinski_harabasz_score`) и индекс Дэвиса — Болдина (`davies_bouldin_score`).\n","\n","Рекомендуем вынести код для построения моделей кластеризации и подбора их параметров в отдельную функцию, чтобы не множить одинаковый код для четырёх случаев дескрипторов.\n","\n","**Примечание.** Поскольку исходных данных много, могут возникнуть проблемы с оперативной памятью и скоростью работы таких алгоритмов, как K-Means. Вместо стандартного алгоритма K-Means можно воспользоваться реализацией MiniBatchKMeans. \n","\n","**Примечание.** Постарайтесь написать чистый код, максимально уменьшая количество дублирующихся участков."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"AGDpbo0WVLuj"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-18 {color: black;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans()</pre></div></div></div></div></div>"],"text/plain":["KMeans()"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["kmeans = KMeans(n_clusters=3)\n","kmeans.fit(data_keeper.active_descriptor_norm)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["array([68174, 75254, 64729])"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["_, cnt = cp.unique(kmeans.labels_, return_counts=True)\n","cnt"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AgglomerativeClustering()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AgglomerativeClustering</label><div class=\"sk-toggleable__content\"><pre>AgglomerativeClustering()</pre></div></div></div></div></div>"],"text/plain":["AgglomerativeClustering()"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["agg = AgglomerativeClustering(n_clusters=5)\n","agg.fit(data_keeper.active_descriptor_std)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[I] [19:54:06.232365] Unused keyword parameter: min_pts during cuML estimator initialization\n"]},{"data":{"text/html":["<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DBSCAN()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DBSCAN</label><div class=\"sk-toggleable__content\"><pre>DBSCAN()</pre></div></div></div></div></div>"],"text/plain":["DBSCAN()"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["dbscan = DBSCAN(min_pts=100, eps=20)\n","dbscan.fit(data_keeper.active_descriptor_std)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["array([  8252, 199905])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["_, cnts = cp.unique(dbscan.labels_, return_counts=True)\n","cnts"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["array([-1,  0], dtype=int32)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["_"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["array([208153,      1,      1,      1,      1])"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["_, cnts = cp.unique(agg.labels_, return_counts=True)\n","cnts"]},{"cell_type":"markdown","metadata":{"id":"fWc9TzQiVLuj"},"source":["### 3.2. Интерпретация кластеров"]},{"cell_type":"markdown","metadata":{"id":"gA7EmN23VLuj"},"source":["#### 3.2.1 Визуализация кластеров"]},{"cell_type":"markdown","metadata":{"id":"PTy2d8C8VLuj"},"source":["Визуализируйте результаты кластеризации в двух- или трёхмерном пространстве, предварительно понизив размерность дескрипторов изображений до соответствующих размерностей с помощью метода t-SNE. \n","\n","По результатам визуализации кластеров сделайте предположение о качестве полученной кластеризации."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvBP0a0nVLuj"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0MQxYxKrVLuj"},"source":["#### 3.2.2. Визуализация изображений в кластере\n"]},{"cell_type":"markdown","metadata":{"id":"ainT7rvqVLuk"},"source":["Визуализируйте несколько изображений из каждого кластера, чтобы проинтерпретировать результаты.\n","\n","**Как визуализировать изображения, соответствующие определённому кластеру?**\n","\n","Мы не рассматривали работу с изображениями как отдельную тему, однако не волнуйтесь — в этом нет ничего страшного.\n","\n","В стандартных библиотеках для визуализации, которые мы изучали ранее, есть встроенный функционал для чтения и визуализации изображений. Например, в библиотеке matplotlib есть функция `plt.imread()`, которая позволяет читать изображение по переданному пути. Она возвращает numpy-массив размерности (h, w, c), где:\n","\n","* h — высота изображения, \n","* w — его ширина,\n","* c — количество каналов.\n","\n","Так как все изображения в нашем датасете цветные, каналов (c) три:\n","\n","* R — матрица интенсивности пикселей красного цвета,\n","* G — матрица интенсивности пикселей зелёного цвета,\n","* B — матрица интенсивности пикселей синего цвета.\n","\n","Например, вот так можно прочитать изображение 000001.jpg:\n","\n","```python\n","img = plt.imread('raw_data/veriwild/1/00001/000001.jpg')\n","print(img.shape)\n","## (557, 756, 3)\n","```\n","\n","То есть изображение состоит из трёх матриц (R, G и B) с размерностью 557 строк на 756 столбцов. Элементами каждой из матриц являются интенсивности пикселей (от 0 до 255) соответствующего цвета.\n","\n","Что касается вывода изображений на экран, в библиотеке matplotlib есть встроенная функция `plt.imshow()`, которая позволяет вывести переданное ей в аргументы изображение:\n","\n","```python\n","fig = plt.figure(figsize=(5, 5))\n","plt.imshow(img);\n","```\n","\n","Функцию `imshow()` можно вызывать и от имени координатных плоскостей при использовании `subplots` из библиотеки `matplotlib`:\n","\n","```python\n","img1 = plt.imread('raw_data/veriwild/1/00001/000001.jpg')\n","img2 = plt.imread('raw_data/veriwild/1/00001/000002.jpg')\n","fig, axes = plt.subplots(1, 2, figsize=(5, 5))\n","axes[0].imshow(img1);\n","axes[1].imshow(img2);\n","```\n","\n","После кластеризации для интерпретации результатов вам понадобится визуализировать несколько изображений из каждого кластера. Для этого мы подготовили функцию `plot_sample_cluster_images()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOdXHUo4VLuk"},"outputs":[],"source":["def plot_samples_images(data, cluster_label, nrows=3, ncols=3, figsize=(12, 5)):\n","    \"\"\"Функция для визуализации нескольких случайных изображений из кластера cluster_label.\n","    Пути до изображений и метки кластеров должны быть представлены в виде DataFrame со столбцами \"paths\" и \"cluster\".\n","\n","\n","    Args:\n","        data (DataFrame): таблица с разметкой изображений и соответствующих им кластеров.\n","        cluster_label (int): номер кластера изображений.\n","        nrows (int, optional): количество изображений по строкам таблицы (по умолчанию 3).\n","        ncols (int, optional): количество изображений по столбцам (по умолчанию 3).\n","        figsize (tuple, optional): размер фигуры (по умолчанию (12, 5)).\n","    \"\"\"\n","    # Фильтруем данные по номеру кластера\n","    samples_indexes = np.array(data[data['cluster'] == cluster_label].index)\n","    # Перемешиваем результаты\n","    np.random.shuffle(samples_indexes)\n","    # Составляем пути до изображений\n","    paths = data.loc[samples_indexes, 'paths']\n","   \n","    # Создаём фигуру и набор координатных плоскостей\n","    fig, axes = plt.subplots(nrows,ncols)\n","    # Устанавливаем размер фигуры\n","    fig.set_size_inches(*figsize)\n","    # Устанавливаем название графика\n","    fig.suptitle(f\"Images from cluster {cluster_label}\", fontsize=16)\n","    # Создаём цикл по строкам в таблице с координатными плоскостями\n","    for i in range(nrows):\n","        # Создаём цикл по столбцам в таблице с координатными плоскостями\n","        for j in range(ncols):\n","            # Определяем индекс пути до изображения\n","            path_idx = i * ncols + j\n","            if path_idx >= len(paths):\n","                break\n","            # Извлекаем путь до изображения\n","            path = paths.iloc[path_idx]\n","            # Читаем изображение\n","            img = plt.imread(path)\n","            # Отображаем его на соответствующей координатной плоскости\n","            axes[i,j].imshow(img)\n","            # Убираем пометки координатных осей\n","            axes[i,j].axis('off')\n"]},{"cell_type":"markdown","metadata":{"id":"iC73rnTGVLuk"},"source":["Например, вы произвели кластеризацию и записали пути до изображений в виде столбца \"paths\" и метки кластеров в виде столбца \"cluster\" в некоторый DataFrame с именем data. Тогда, чтобы визуализировать несколько случайных изображений из кластера 0, вам нужно вызвать функцию `plot_sample_cluster_images()` следующим образом:\n","\n","```python\n","plot_samples_images(data=data, cluster_label=0)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6Kk3GmJ-hnr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6SGnhYzGVLul"},"source":["### 3.3. Поиск выбросов"]},{"cell_type":"markdown","metadata":{"id":"-QYxsfN9VLul"},"source":["С помощью известных вам методов поиска выбросов (например, DBSCAN) попытайтесь найти выбросы среди изображений, используя все варианты дескрипторов. Подберите параметры алгоритма.\n","\n","Визуализируйте изображения, попавшие в раздел выбросов, и попробуйте проинтерпретировать полученные результаты. Подумайте, почему именно эти изображения попали в выбросы.\n","\n","Сравните результаты для всех вариантов дескрипторов. Какой вариант дескрипторов даёт наилучшее представление о выбросах?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bVbQemn-j8-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"D8hfxVH6VLul"},"source":["## 4. Выводы и оформление проекта"]},{"cell_type":"markdown","metadata":{"id":"738CHyLvVLul"},"source":["На основе результатов, полученных при выполнении проекта, сделайте вывод по задаче, приведя таблицу со сравнением результатов кластеризации на каждом из наборов дескрипторов. Приведите сравнение вариантов предобработки исходных данных по качеству кластеризации.\n","\n","Результатом вашей работы должно стать небольшое исследование, в котором вы даёте команде IntelliVision рекомендации, какие дескрипторы, с какой предобработкой и каким алгоритмом кластеризации лучше всего подходят для решения задачи.\n","\n","Также сохраните результаты лучшего алгоритма в CSV-файл со столбцами path (путь до изображения) и cluster (номер кластера). В описании к проекту приведите расшифровку каждого из кластеров.\n","\n","Когда вы закончите выполнять проект, создайте в своём репозитории файл README.md и кратко опишите содержание проекта по принципу, который мы приводили ранее.\n","\n","Выложите свой проект на GitHub и оформите удалённый репозиторий, добавив в него описание и теги (придумайте их самостоятельно в зависимости от того, какую задачу вы решали)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f97-yQJTVLul"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"86c56a74836ad344b00594bf6f38fa6a676a207ceefe20d101fbc465800ccb8d"}}},"nbformat":4,"nbformat_minor":0}
